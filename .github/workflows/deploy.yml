name: GitHub Pages

on:
  workflow_dispatch:
  push:
    branches:
      - main

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  deploy:
    concurrency:
      group: "pages"
      cancel-in-progress: false
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v3

      - name: Setup Node
        uses: actions/setup-node@v3
        with:
          node-version: "18"

      - name: Get yarn cache
        id: yarn-cache
        run: echo "YARN_CACHE_DIR=$(yarn cache dir)" >> "${GITHUB_OUTPUT}"

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ${{ steps.yarn-cache.outputs.YARN_CACHE_DIR }}
          key: ${{ runner.os }}-website-${{ hashFiles('**/yarn.lock') }}
          restore-keys: |
            ${{ runner.os }}-website-
      - name: Cache Docusaurus Build
        uses: actions/cache@v3
        with:
          path: node_modules/.cache/webpack
          key: docusaurus-${{ github.ref_name }}-${{ github.run_id }}
          restore-keys: |
            docusaurus-${{ github.ref_name }}
            docusaurus-main

      - run: yarn install --frozen-lockfile
      - run: npm run build
        env:
          NODE_ENV: "production"
          NODE_OPTIONS: --max-old-space-size=12288
      - name: Setup Pages
        uses: actions/configure-pages@v3
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v2
        with:
          path: "./build"
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v2

  # NB: We use a separate job so we don't hog the single spot dedicated to building/deploying
  # (since we set `concurrency` on that job). As more things merge, they can start building/deploying
  # while this spins.
  reindex:
    runs-on: ubuntu-22.04
    needs: deploy
    steps:
      - name: Wait for deployment to propagate
        run: |
          TIMEOUT_S=300
          SLEEP_S=5

          while [ $TIMEOUT_S -gt 0 ]; do
            if curl -s http://www.pantsbuild.org | grep -q "$GITHUB_SHA"; then
              echo "Found ref! Continuing on."
              break
            fi

            echo "Ref not found yet, sleeping for $SLEEP_S seconds"
            sleep $SLEEP_S
            TIMEOUT_S=$((TIMEOUT_S-SLEEP_S))
          done

          if [ $TIMEOUT_S -le 0 ]; then
            echo "TIMEOUT_S reached, failing!"
            echo "::error::Timeout waiting for deploy"
            exit 1
          fi

      # See https://www.algolia.com/doc/rest-api/crawler/#reindex-with-a-crawler
      - name: Kickoff a crawl
        run: |
          curl \
            -H "Content-Type: application/json" \
            -X POST \
            --user ${{ secrets.THEJCANNON_ALGOLIA_CRAWLER_USER_ID }}:${{ secrets.THEJCANNON_ALGOLIA_CRAWLER_API_KEY }} \
            https://crawler.algolia.com/api/1/crawlers/7ae90af1-f627-4806-a2cc-89e7157daa44/reindex
